{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486b6ecf-15e1-41ff-93cc-6f15628ee631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from monai.networks.schedulers import DDPMScheduler\n",
    "from monai.inferers.inferer import DiffusionInferer\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import pandas as pd\n",
    "from monai.bundle import ConfigParser\n",
    "import nibabel as nib\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#%% Setup device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df6865ff-7b80-4775-a956-79fc4e311972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths to uploaded training data\n",
    "training_dataset = 'TrainingData_Key/slices_40_anon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f79ca9f-7fae-46ea-bddd-a9c985bfc10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths to uploaded config files\n",
    "config_path_ = 'Configs/configs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9422838a-dcd4-41f2-be3b-1f298b7a1e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = 'trained_vAE_epoch_148.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4708c5b8-0c99-4c6c-a22e-32a594ecd3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(df, data_dir):\n",
    "\n",
    "    df['Linked_Files_Anon'] = df['Linked_Files_Anon'].apply(lambda x: os.path.join(data_dir, x))\n",
    "\n",
    "    train_data = df[df['Set']=='Train']\n",
    "    valid_data = df[df['Set']=='Validation']\n",
    "    test_data = df[df['Set']=='Test']\n",
    "\n",
    "    train_data = train_data.reset_index()\n",
    "    valid_data = valid_data.reset_index()\n",
    "    test_data = test_data.reset_index()\n",
    "\n",
    "    train_dataset = FDG_Dataset(data=train_data)\n",
    "    valid_dataset = FDG_Dataset(data=valid_data)\n",
    "    test_dataset = FDG_Dataset(data=test_data)\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b2c331-fa06-4931-8c43-cd0e0a9cbde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FDG_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset to load NIfTI files from a provided list of file paths.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_paths (list of Path objects): List of paths to the NIfTI files.\n",
    "        \"\"\"\n",
    "\n",
    "        self.csv = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        diag = self.csv.loc[idx, 'DX_encoded']\n",
    "        file_name = self.csv.loc[idx, 'Linked_Files_Anon']\n",
    "\n",
    "        nii_img  = nib.load(file_name)\n",
    "        nii_data = nii_img.get_fdata()\n",
    "\n",
    "        nii_data_scaled = (nii_data - nii_data.min())/(nii_data.max() - nii_data.min())\n",
    "        image_tensor = torch.from_numpy(nii_data_scaled)\n",
    "\n",
    "        return image_tensor, diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5600518a-2f7c-4f1e-9158-b17eb14c7d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1-2): 2 x AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (3): AEKLDownsample(\n",
       "        (pad): AsymmetricPad()\n",
       "        (conv): Convolution(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (4): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Convolution(\n",
       "          (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (6): AEKLDownsample(\n",
       "        (pad): AsymmetricPad()\n",
       "        (conv): Convolution(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (7): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Convolution(\n",
       "          (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (8-9): 2 x AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (10): SpatialAttentionBlock(\n",
       "        (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (attn): SABlock(\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (to_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (to_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (to_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (qkv): Identity()\n",
       "          (input_rearrange): Rearrange('b h (l d) -> b l h d', l=1)\n",
       "          (out_rearrange): Rearrange('b l h d -> b h (l d)')\n",
       "          (drop_output): Dropout(p=0.0, inplace=False)\n",
       "          (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (12): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "      (13): Convolution(\n",
       "        (conv): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv2d(1, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (2): SpatialAttentionBlock(\n",
       "        (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (attn): SABlock(\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (to_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (to_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (to_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (qkv): Identity()\n",
       "          (input_rearrange): Rearrange('b h (l d) -> b l h d', l=1)\n",
       "          (out_rearrange): Rearrange('b l h d -> b h (l d)')\n",
       "          (drop_output): Dropout(p=0.0, inplace=False)\n",
       "          (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3-5): 3 x AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (6): UpSample(\n",
       "        (upsample_non_trainable): Upsample(scale_factor=(2.0, 2.0), mode='nearest')\n",
       "        (postconv): Convolution(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (7): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Convolution(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (8): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (9): UpSample(\n",
       "        (upsample_non_trainable): Upsample(scale_factor=(2.0, 2.0), mode='nearest')\n",
       "        (postconv): Convolution(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (10): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Convolution(\n",
       "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (11): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (12): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "      (13): Convolution(\n",
       "        (conv): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (quant_conv_mu): Convolution(\n",
       "    (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (quant_conv_log_sigma): Convolution(\n",
       "    (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (post_quant_conv): Convolution(\n",
       "    (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config filename\n",
    "config_file=\"train_autoencoder.json\"\n",
    "\n",
    "# Weights Filename\n",
    "weights_file=\"trained_vAE_epoch_148.pt\"\n",
    "\n",
    "# Setup config filepath\n",
    "config_path = os.path.join(config_path_, config_file)\n",
    "\n",
    "# Setup weights filepath\n",
    "weights_path = os.path.join(pretrained_path, weights_file)\n",
    "\n",
    "# Read config\n",
    "config = ConfigParser()\n",
    "config.read_config(config_path)\n",
    "\n",
    "# Parse model\n",
    "vAE_model = config.get_parsed_content(\"gnetwork\")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(weights_path, map_location=device)\n",
    "\n",
    "# Key remapping\n",
    "key_mapping = {\n",
    "    \"encoder.blocks.10.to_q.weight\": \"encoder.blocks.10.attn.to_q.weight\",\n",
    "    \"encoder.blocks.10.to_q.bias\": \"encoder.blocks.10.attn.to_q.bias\",\n",
    "    \"encoder.blocks.10.to_k.weight\": \"encoder.blocks.10.attn.to_k.weight\",\n",
    "    \"encoder.blocks.10.to_k.bias\": \"encoder.blocks.10.attn.to_k.bias\",\n",
    "    \"encoder.blocks.10.to_v.weight\": \"encoder.blocks.10.attn.to_v.weight\",\n",
    "    \"encoder.blocks.10.to_v.bias\": \"encoder.blocks.10.attn.to_v.bias\",\n",
    "    \"encoder.blocks.10.proj_attn.weight\": \"encoder.blocks.10.attn.out_proj.weight\",\n",
    "    \"encoder.blocks.10.proj_attn.bias\": \"encoder.blocks.10.attn.out_proj.bias\",\n",
    "    \"decoder.blocks.2.to_q.weight\": \"decoder.blocks.2.attn.to_q.weight\",\n",
    "    \"decoder.blocks.2.to_q.bias\": \"decoder.blocks.2.attn.to_q.bias\",\n",
    "    \"decoder.blocks.2.to_k.weight\": \"decoder.blocks.2.attn.to_k.weight\",\n",
    "    \"decoder.blocks.2.to_k.bias\": \"decoder.blocks.2.attn.to_k.bias\",\n",
    "    \"decoder.blocks.2.to_v.weight\": \"decoder.blocks.2.attn.to_v.weight\",\n",
    "    \"decoder.blocks.2.to_v.bias\": \"decoder.blocks.2.attn.to_v.bias\",\n",
    "    \"decoder.blocks.2.proj_attn.weight\": \"decoder.blocks.2.attn.out_proj.weight\",\n",
    "    \"decoder.blocks.2.proj_attn.bias\": \"decoder.blocks.2.attn.out_proj.bias\",\n",
    "    \"decoder.blocks.6.conv.conv.weight\": \"decoder.blocks.6.postconv.conv.weight\",\n",
    "    \"decoder.blocks.6.conv.conv.bias\": \"decoder.blocks.6.postconv.conv.bias\",\n",
    "    \"decoder.blocks.9.conv.conv.weight\": \"decoder.blocks.9.postconv.conv.weight\",\n",
    "    \"decoder.blocks.9.conv.conv.bias\": \"decoder.blocks.9.postconv.conv.bias\",\n",
    "}\n",
    "\n",
    "# Remap keys\n",
    "new_state_dict = {key_mapping.get(k, k): v for k, v in checkpoint.items()}\n",
    "\n",
    "# Load state\n",
    "vAE_model.load_state_dict(new_state_dict, strict=False)\n",
    "vAE_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ab136fd-9a8c-4bd1-99df-8aaf1388f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config file\n",
    "config_file=\"train_diffusion.json\"\n",
    "\n",
    "# Setup config filepath\n",
    "config_path = os.path.join(config_path_, config_file)\n",
    "\n",
    "# Read config\n",
    "config = ConfigParser()\n",
    "config.read_config(config_path)\n",
    "\n",
    "# Parse model\n",
    "LDM_model = config.get_parsed_content(\"diffusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eae0077-0275-4e79-b82b-a00621267693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets / Dataloader\n",
    "data = os.path.join('TrainingData_Key/data_key_anon.csv')\n",
    "data = pd.read_csv(data)\n",
    "train_dataset, valid_dataset, test_dataset = create_datasets(data, training_dataset)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4, persistent_workers=True)\n",
    "\n",
    "# Epochs\n",
    "training_epochs = 500\n",
    "\n",
    "# Learning rate\n",
    "init_lr = 5e-05\n",
    "fin_lr = 1e-08\n",
    "# Optimizer\n",
    "optimiser = optim.Adam(LDM_model.parameters(), lr=init_lr)\n",
    "# Learning rate scheduler\n",
    "#lr_scheduler = CombinedScheduler(optimiser, 1, training_epochs, 20, init_lr, fin_lr)\n",
    "\n",
    "# LDM Scheduler\n",
    "ldm_scheduler = DDPMScheduler(schedule=\"scaled_linear_beta\", num_train_timesteps=1000, beta_start=0.0015, beta_end= 0.0195)\n",
    "\n",
    "# Inferer\n",
    "inferer = DiffusionInferer(ldm_scheduler)\n",
    "\n",
    "# Output folder\n",
    "output_folder = '/content/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c0e197b-c044-408b-bebc-00fab88896a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDM_state_dict = torch.load('Checkpoints/trained_LDM_epoch_389.pt', mmap=True, weights_only=True)\n",
    "LDM_model.load_state_dict(LDM_state_dict, assign=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f63b33b-d333-425f-98e4-3bc00a672257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1501088/1259945237.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=True):\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:03<00:00, 15.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:03<00:00, 15.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:03<00:00, 15.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:03<00:00, 15.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:03<00:00, 15.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:03<00:00, 15.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:03<00:00, 15.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:03<00:00, 15.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:03<00:00, 15.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:03<00:00, 15.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 1, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(4046)\n",
    "\n",
    "save_dir = \"./synthetic_non_conditional\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "vAE_model.eval()\n",
    "LDM_model.eval()\n",
    "\n",
    "np_imgs = np.zeros([1, 1, 240, 240])\n",
    "np_vAE_imgs = np.zeros([1, 1, 240, 240])\n",
    "\n",
    "for i in range(10):\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn((128, 1, 64, 64))\n",
    "        noise = noise.to(device)\n",
    "        ldm_scheduler.set_timesteps(num_inference_steps=1000)\n",
    "        with autocast(enabled=True):\n",
    "            image = inferer.sample(input_noise=noise, diffusion_model=LDM_model, scheduler=ldm_scheduler)\n",
    "            \n",
    "            image = image[:, :, 2:-2, 2:-2]\n",
    "            image = vAE_model.decode(image).cpu().detach().numpy()\n",
    "            \n",
    "            noise = noise[:, :, 2:-2, 2:-2]\n",
    "            vAE_image = vAE_model.decode(noise).cpu().detach().numpy()\n",
    "            \n",
    "            np_vAE_imgs = np.concatenate((np_vAE_imgs, vAE_image), axis=0)\n",
    "            np_imgs = np.concatenate((np_imgs, image), axis=0)\n",
    "\n",
    "        for j in range(image.shape[0]):\n",
    "            img = image[j]  # Shape: [C, H, W]\n",
    "            # Convert to [H, W] or [H, W, C] if needed\n",
    "            if img.shape[0] == 1:\n",
    "                img = img[0]  # [H, W]\n",
    "            else:\n",
    "                img = np.transpose(img, (1, 2, 0))  # [H, W, C]\n",
    "\n",
    "            \n",
    "            nifti_img = nib.Nifti1Image(np.float32(img), affine=np.eye(4))\n",
    "            nib.save(nifti_img, f\"{save_dir}/img_{i}_{j}.nii.gz\")\n",
    "\n",
    "fake_images2 = np_imgs[1:]\n",
    "vAE_images = np_vAE_imgs[1:]\n",
    "print(np.shape(fake_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c4bf68-c7fd-4109-abd0-eddbd2c84cdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1262, 1, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = 'TrainingData_Key/slices_40_anon/'\n",
    "csv = pd.read_csv('TrainingData_Key/data_key_anon.csv')\n",
    "\n",
    "np_imgs = np.zeros([1,1,240,240])\n",
    "\n",
    "for idx, row in csv.iterrows():\n",
    "    file_name = row['Linked_Files_Anon']\n",
    "    file_name = os.path.join(path, file_name)\n",
    "    nii_img = nib.load(file_name)\n",
    "    real_images = nii_img.get_fdata()\n",
    "    np_imgs = np.concatenate((np_imgs, np.expand_dims(np.expand_dims(real_images, axis=0), axis=0)), axis=0)\n",
    "\n",
    "real_images = np_imgs[1:]\n",
    "print(np.shape(real_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c24fa62-1900-4d23-8c35-17350fb2a86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 1, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(fake_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d10e21c1-77e3-431a-b740-0d75a458b687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 0.32708024978637695\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "fid = FrechetInceptionDistance(normalize=True)\n",
    "fid.update(torch.tensor(fake_images).repeat(1,3,1,1), real=True)\n",
    "fid.update(torch.tensor(fake_images2).repeat(1,3,1,1), real=False)\n",
    "\n",
    "print(f\"FID: {float(fid.compute())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
